[
  {
    "objectID": "posts/2021-05-09-not-knowing/2021-05-09-not-knowing.html",
    "href": "posts/2021-05-09-not-knowing/2021-05-09-not-knowing.html",
    "title": "Not Knowing",
    "section": "",
    "text": "I don’t have any children, but if I did I think I’d be one of those parents who keeps kicking the naming can down the road and referring to their newborn as “the baby” for weeks into its life. Point being, I think naming something is really hard, especially if it’s something young and new that hasn’t quite fully formed. I overthink and second guess, I make spreadsheets of options and weigh the pros and cons, and in the end I can never be 100% satisfied with the result. I also think this defficiency is not unique to me in my profession, for it seems many practicioners of science struggle to creatively christen their creations. I think if academic papers were required to have short, uncomplicated titles, there would be a lot less getting published every year.\nIn the epic struggle of picking a name for this blog, I settled on a reference to the famous quote by Isaac Newton:\n\nIf I have seen further than others, it is by standing upon the shoulders of giants.\n\nI think this quote captures the essense of what I’m trying to do with this blog. It’s a powerful metaphor, the idea that science allows us to reach far beyond our individual acuities, to glimpse profound truths on the horizon of human understanding, but only because we are elevated by the insight and hard work of generations of past thinkers. It’s beautiful, in a way, to imagine perching precariously on the shoulder of some enormous creature, gazing at a vast landscape that would be hidden to those on the ground.\nBut it’s also a bit… grandiose? Pretentious? Who am I, after all, to suggest that my shoulders are worth standing on? I’m no giant, no leading expert in any field. I’m not even a full-fledged Ph.D. yet. And thus the appropriate version of this metaphor became apparent to me. I’m not an authority on science or mathematics, or anything else for that matter. But I’m a student, an aspiring shoulder-gazer. I’m still standing on the ground, looking up at the giant and wondering about the view.\nSo by reading this blog you won’t necessarily get the perspecive of a giant’s shoulders, but you can, if you like, come along with me while I start to climb. Every day we’ll get a little higher and see a little further. I hope to make a career some day out of science communication, so maybe this is the natural place to start, at the giant’s feet, climbing gear in hand. Maybe some day we can look out at the horizon together."
  },
  {
    "objectID": "posts/2021-05-09-not-knowing/2021-05-09-not-knowing.html#a-little-time-to-change",
    "href": "posts/2021-05-09-not-knowing/2021-05-09-not-knowing.html#a-little-time-to-change",
    "title": "Not Knowing",
    "section": "A Little Time to Change",
    "text": "A Little Time to Change\nThere’s another problem with Newton’s quote about the giants, besides it being a bit lofty. It’s basic. Many of you I’m sure have heard the quote before. It’s appeared numerous times across popular culture, from song lyrics to sitcoms. Stephen Hawking wrote a whole book called On the Shoulders of Giants: The Great Works of Physics and Astronomy. The official motto of Google Scholar is “Stand on the Shoulders of Giants”, and the video game Portal 2 humorously subverts the metaphor through the scientific arrogance of the fictitious Aperture Science. It’s even inscribed on the edge of the British two pound coin.\nPerhaps it’s the dormant Austin hipster in me that I pretend isn’t there, but surely you don’t want something too popular as the source for your name. It’s not cool unless it’s sufficiently obscure, right? Well there is another quote, one that I think even better represents my attitude toward this blog (and also probably toward life in general), and it is at least slightly more obscure:\n\nI was born not knowing and have had only a little time to change that here and there.\n– Richard Feynman\n\nIt’s hard to convey just how much this sentiment resonates with me. It’s direct and unpretentious, and it boils all of the complexity of life down to something simple but true: we are born ignorant, and every day we gather data to update our knowledge of the world. Every exam taken, every book or social media post read, every conversation had, and every sight seen and sound heard contributes to this endeavor. And we are all equal in this way, all born the same and then thrust into the world to absorb as much as we can before our time is up.\nBut alas, partly due to the simplicty of its language that drew me to it, this quote does not contain a catchy two or three word phrase that can be synthesized into the name of a blog. Trust me, I tried. I settled on referencing it in the title of this post, but that’s about all I could milk out of it. So instead you get the story about the giants and the legacy of Isaac Newton and every scientist, songwriter, and video game developer who’s quoted him over the years.\nOh well, I can live with being a little basic, especially since this was all just an excuse to ramble about interesting quotes and the several sources of inspiration I had for starting this blog. Maybe that’s the best summary of what to expect: me looking for excuses to rant about things I find interesting.\nDid you know Newton didn’t even come up with the shoulders of giants thing? When he mentioned it in a letter to Robert Hooke in 1676, he was referencing a metaphor that was already quite popular at the time, one which apparently can be traced to Bernard of Chartres, a 12th century French philosopher, who had in mind the image of dwarfs riding on the shoulders of giants from ancient Greek mythology. I wonder what Bernard would think of all the credit Newton gets for referencing his metaphor."
  },
  {
    "objectID": "posts/catGPT/index.html",
    "href": "posts/catGPT/index.html",
    "title": "CatGPT: Part 1",
    "section": "",
    "text": "Well, I’m back. And after nearly two years of no activity on this blog, what incredible spectacle do I have prepared for you, my irrationally loyal reader? Introducing CatGPT, the Catenated Grimalkin Program Talker. As stupid as it sounds, I spent a lot of time with the thesaurus to come up with that name, so please let me have this. Naming things is hard.\nCatGPT is a simple AI chat bot that uses Markov chain-based text prediction to generate responses, and it seems to have suspiciously specific knowledge about just one thing… cats. So like, do you get it now? Grimalkin is an old word for cat, you see. And if you think that’s silly then you try googling “synonyms for cat that start with G, P, or T” and see if you can do better. On second thought, you might want to skip the “P” search. Anyway, catenate is a word containing “cat” that also happens to mean “to link together”, which fits well with the Markov chain theme, so yea, this name is actually very clever, and it’s you who’s spent too much time thinking about it.\nWhere was I? Oh yes, CatGPT isn’t just a blatant attempt to capitalize on the current popularity of some other chat bots out there, it’s one that we’re going to build together! I guess to be more clear, this is a tutorial where I’ll walk you through how to build a text generation tool using simple Markov chains and dress it up a bit to be something we can talk to. Obviously we won’t be blowing any LLMs out of the water here, but hopefully we’ll learn some things along the way about probability, natural language, and Python development."
  },
  {
    "objectID": "posts/catGPT/index.html#markov-chains-and-language",
    "href": "posts/catGPT/index.html#markov-chains-and-language",
    "title": "CatGPT: Part 1",
    "section": "Markov chains and language",
    "text": "Markov chains and language\nIn a fairly abstract sense, text generation comes down to answering the question: how do I decide what to say next based on what was just said? Language, after all, is not random; it contains structure (grammar, syntax, diction) that emerges as correlations in the text. Certain characters are more likely to follow certain other characters, which are more likely to be arranged in certain orders and appear in certain places, and so on. This correlation structure is what allows language to carry information, such that the reader can parse that information using his or her own understanding of the structure. In general, natural human language is messy and imprecise, and the actual information content of a piece of text is highly complicated, involving correlations that aren’t necessarily local. One of the things that makes ChatGPT so powerful is its ability to capture and reproduce these correlations at many scales, allowing for text that feels holistic and sensible from start to finish.\nWe, however, will start from a humble place and focus on the kind of text generation where what we say will depend only on what was just said, roughly speaking. It is precisely this property that will make our model Markovian, i.e. the probability that we produce a character \\(c\\) will depend only on the previous \\(k\\) characters in the text. You might think that we need to restrict ourselves to looking at only one previous character to be truly Markovian, but it turns out the number of characters doesn’t really matter (in fact, we could do the same thing with whole words or even more abstract “tokens”, if our source text were rich enough). What matters is that our next character is influenced by a sequence of previous ones, not that sequence and some other sequence from a couple paragraphs back, for example. In this way, one sequence influences the next sequence, which influences the next sequence, and so on such that all the sequences of length \\(k\\) are probabilistically linked together, which is what makes this a Markov chain.\nSo, we’ll call \\(k\\) the kernel size and call the output (single character) of our model \\(y\\). Then, given an input \\(x\\) that is a string of \\(k\\) characters, we want to compute the probability \\[ P(y = c | x)\\quad \\forall c\\in\\mathcal{A} \\] Here, \\(\\mathcal{A}\\) is the alphabet for our model, the set of all the possible characters we’re allowed to produce. In our case this happens to also be all the unique characters that appear in our source text. Once we have this probability in hand, we’re golden. For a given \\(x\\), we have a probability distribution over all possible choices of \\(c\\). All we need to do is pick a character at random according to that probability, spit it out, and repeat.\nThe trick, of course, is that we need to somehow compute this probability. In our case, this will be done by simply counting occurences in the original text, such that our generated text will have the same correlation structure as the source, for whatever \\(k\\) is pre-selected."
  },
  {
    "objectID": "posts/catGPT/index.html#developing-the-code",
    "href": "posts/catGPT/index.html#developing-the-code",
    "title": "CatGPT: Part 1",
    "section": "Developing the code",
    "text": "Developing the code\nFor this project, we’re going to use Python (for now). It’s worth noting that there are powerful tools out there for manipulating text and processing natural language, such as NLTK, but we’re going to mostly avoid those for this project and do things ourselves.1 We’ll start by building up the three main ingredients of our model:\n\nBuilding the probability distribution from the source text\nGenerating new text\nMaking it interactive\n\n\nBuilding the distribution\nThere are different ways to go about this, but the first idea I had turned out to be easy to implement and perfectly adequate in efficiency. The idea is to use nested dictionaries to create a lookup table for character frequencies. The outer dictionary’s keys will be sequences of \\(k\\) characters, each one pointing to an inner dictionary of characters pointing to their frequencies.\ndef build_lookup(source_text, k):\n    lookup = dict()                     # the outer dictionary\n\n    for i,_ in enumerate(source_text):\n        if i+k >= len(source_text):     # make sure you don't go out of bounds\n            break\n\n        x = source_text[i:i+k]          # x, the current sequence of k chars\n        y = source_text[i+k]            # y, the next char that appears\n\n        if x not in lookup:             # if this is the first time we've seen x, add it\n            lookup[x] = {y: 1}\n        else:\n            if y not in lookup[x]:      # count the number of times y appears after x\n                lookup[x][y] = 1\n            else:\n                lookup[x][y] += 1\n    \n    return lookup\nThe loop creates a sliding window, our \\(k\\)-sized kernel, that scans over the source text and counts up the number of times different characters follow. Now, when we access this lookup dictionary with some input x, what we get back is the probability distribution we were after. Of course, really what we get is a distribution of frequencies, and we’d need to divide these by the total number of occurences to get probabilities, but this is an unnecessary step, as we’ll see later. Python’s random.choices function allows us to use the raw frequencies as weights to get the desired result.\n\n\nGenerating new text\nIt almost seems too good to be true, but generating new text is similarly pretty straightforward. What we need is a desired length for the new text, a seed from which to start adding characters, and the lookup table that we built from before. We just pick new characters from the lookup inner dictionary weighted by their frequencies (which are the values in that dictionary).\nimport random\n\ndef generate_text(lookup, k, length, seed):\n    text = seed\n\n    for i in range(k, length):\n        x = text[i-k:]                      # current input, starting with seed\n\n        chars = list(lookup[x].keys())      # the chars to choose from\n        weights = list(lookup[x].values())  # the frequencies with which those chars appeared in source\n\n        next_char = random.choices(chars, weights=weights, k=1)[0]\n        text += next_char\n    \n    return text\nIt’s really that simple. By the way, random.choices returns a list, so with k=1 we’re telling it to return one character, but we still need to grab the \\(0\\)th element to access it.\nNow would be a great time to run a little test to see if things work. For now, let’s take just the first paragraph from the Wikipedia page for cat and use that as source text.\nsource_text = 'The cat (Felis catus) is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae and is commonly referred to as the domestic cat or house cat to distinguish it from the wild members of the family. Cats are commonly kept as house pets but can also be farm cats or feral cats; the feral cat ranges freely and avoids human contact. Domestic cats are valued by humans for companionship and their ability to kill rodents. About 60 cat breeds are recognized by various cat registries.'\n    \nk = 1\n\nlookup = build_lookup(source_text, k)\n\nseed = source_text[:k]\nnew_text = generate_text(lookup, k=k, length=100, seed=seed)\n\nprint(new_text)\nExecuting the above (\\(k=1\\)) gave me:\n\nThed t bus famo inivats It dond avavamby mmis. acon by 60 f tis hegicon tst wie ican d t omarouivous\n\nIf we crank \\(k\\) up a bit, to say \\(k=5\\), we get something a little more English sounding:\n\nThe cat ranges freely and their ability to kill rodents. About 60 cat ranges freely and their abilit\n\nOur bot seems a bit confused and is repeating itself, but that’s just because of how small our source text is. If you’d like, you can throw an even shorter source text in there, like “hello there”, and print out the lookup table to convince yourself of what it’s doing. This can be useful for debugging.2\n\n\nMaking it interactive\nAs a final step, we’d like to make this text generation code something we can actually interact with. In the spirit of mimicking ChatGPT, we envision a setup where we can ask the bot something, and have its response in some way incporporate what we asked. Furthermore, we’d like to rinse and repeat this to have an ongoing conversation.\nLet me be clear: we’re not getting fancy here. The beloved CatGPT that we’re building is… special. The goal here is to be a bit goofy, so we’re going to make this interactive in quite a dumb way. But more on that later.\nFirst, let’s fill CatGPT’s brain with a lot more info, specifically all the Wikipedia pages in the category “Cats”. The general and proper way to scrape data from the web would be to use a combination of the requests and beautifulsoup packages, but we can get away with even more simplicity by using the wikipediaapi package. If we want to extend our bot to learn from other online sources in the future, we’d need to go the former route.\nSo let’s write a function that takes a category name and grabs the content for all the pages that are in that category.\nimport wikipediaapi\n\ndef load_wiki(category):\n    wiki = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI)\n\n    cat_page = wiki.page(category)\n    article_names = [p for p in cat_page.categorymembers.keys() if ':' not in p]\n\n    content = ''\n    for article in article_names:\n        page = wiki.page(article)\n        page_content = page.text\n        content += page_content\n\n    return content.replace('\\n', ' ')\nThe one subtlety to point out here is in the line article_names = [p for p in cat_page.categorymembers.keys() if ':' not in p]. We’re excluding page names that contain “:” in them because we don’t want to deal with the recursive nature of Wikipedia categories. To keep things simple, we’re only going to scrape the pages that sit directly in the “Cats” category, and this line of code seems to do the trick.\nNow the final step. We need to:\n\nBuild our lookup table using the source text scraped from the Wikipedia articles\nAsk the user for some input\nParse the input to extract “keywords” to then search for in the source text\nRandomly select a \\(k\\) sized seed from the text based on this keyword search\nGenerate a response using that seed\nRepeat\n\nOk maybe I got a bit ahead of myself when I said “final step”. Steps 3 and 4 here seem like a head scratcher, so let’s spell out a little more clearly what they mean. Again, I want the user’s input to somehow influence the response by CatGPT, and the simplest way I could think to do this was to search the source text for instances matching words in the user’s input and use that as a seed to generate the response.\nTwo issues immediately arise from this approach. First, we don’t want to waste our time looking for common words and articles, so we’re going to use a dictionary of stopwords from nltk.corpus to filter out such words, leaving us with more interesting “keywords”. Second, we need our seed to be of length \\(k\\), so some padding around the keywords will be needed to make sure whatever sequences we find are the correct length. Then we’ll just pick one at random and go from there.\nFinally, the code to do all this is below. There are some small tricks here and there, like using regex and string translation to filter out punctuation, prepending our keywords with spaces to avoid finding matches in the middle of words, and some others. But instead of going through every trick, I encourage you to take this code and play around with it yourself to uncover all its quirks. Again, for a project like this it is absolutely necessary to continually unit test, try your best to break it with nontrivial inputs, and generally mess around with it until you can shake all the bugs out.\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\ndef run_catGPT():\n    # load text from all the Wikipedia pages on cats\n    source_text = load_wiki('Category:Cats')\n\n    # use nltk to download the set of stopwords\n    nltk.download('stopwords')\n    stops = set(stopwords.words('english'))\n\n    # set kernel size. I'm a fan of k = 10\n    k = 10\n\n    # build probability distribution as lookup table\n    lookup = build_lookup(source_text, k)\n\n    print('\\nHello, my name is CatGPT. What can I help you with today?')\n\n    while True:\n        # request user input, and exit if the user says goodbye\n        input_text = input('\\n> ')\n        if input_text.lower() == 'goodbye':\n            break\n        \n        # get a list of the user's words, ignoring punctuation, then filter out the stopwords\n        words = re.findall(r\"[\\w']+|[.,!?;]\", \n            input_text.lower().translate(str.maketrans('', '', string.punctuation)))\n        keywords = list(filter(lambda w: not w in stops, words))\n\n        # for each keyword, find matches in the text and pad them to be of length k\n        # add them to a list of potential seeds\n        seeds = []\n        for w in keywords:\n            for match in re.finditer(' ' + w, source_text):\n                s = match.start() + 1\n                e = match.end()\n                padded = source_text[s:e]\n                \n                if len(padded) > k:\n                    seeds.append(padded[:k])\n                else:\n                    while len(padded) < k:\n                        e += 1\n                        padded = source_text[s:e]\n                    \n                    seeds.append(padded)\n        \n        # if for some reason no match was found, set the seed as start of text\n        if not seeds:\n            seed = source_text[0:k]\n        else:\n            seed = random.choice(seeds)\n\n        # generate a response of length 500 (or whatever you want)\n        response = generate_text(lookup, k, length=500, seed=seed)\n\n        # do some post-processing to make sure the response ends with a period and starts capitalized\n        last_period = response.rfind('.')\n        if last_period == -1:\n            last_period = len(response)\n            response += '.'\n        response = response[0].upper() + response[1:last_period+1]\n\n        print('\\n' + response)"
  },
  {
    "objectID": "posts/catGPT/index.html#a-conversation-with-catgpt",
    "href": "posts/catGPT/index.html#a-conversation-with-catgpt",
    "title": "CatGPT: Part 1",
    "section": "A conversation with CatGPT",
    "text": "A conversation with CatGPT\nThe following is a conversation I had with CatGPT. Enjoy.\n\n\nHello, my name is CatGPT. What can I help you with today?\n\n\n\nHi CatGPT. Do you have any cat fun facts for me?\n\n\n\nCats and free-ranging domestic and wild cats descended from domestic cat with a large area entirely unsuitable for breeding have red eyes. The point gene allele is a single cat found with intact males, while spayed female cat”, also used to call a cat. The myth is attributed to the slaughter.\n\n\n\nJesus, that got kind of dark at the end\n\n\n\nDarkest of all the boarding catteries. In 2007 market surveys showed that 67% of the mice killed by feral cats may chase. Reproduction. These lineages include pain and swelling around the mid 950s BC onwards. At the times at which a kitten becomes difficulties in distinguishable from domesticated cats. Along with rabbits, some sea birds, and sheep, form the entire growth cycle of contraction and establishment, but the current wildcat population. There are various methods for training the cat.\n\n\n\nI didn’t really understand most of that, but yes training a cat is what I want! I’d love to train my cat\n\n\n\nCats. Young orphaned and how long they were embalmed, coffined and buried in cat cemeteries for mummies offered to Bastet.In the mid 19th century. It was suggested that the earth was experience poor growth and can result in retinal degenerations could reach them only with a small cat species, a factor that has been identified as a domestic cats (mostly unowned) are the top human-caused threat to wildlife, transmission of illness.\n\n\n\nOh, I see. That’s pretty bleak. Also, I’m beginning to suspect you’re not really that smart\n\n\n\nAlso neurological cell types. White cats and other animal welfare. The cat will produce black eumelanin based) fur, caused by the abandonment of pets. The usefulness of TNR are often funded by big businesses to combat the city’s cemetery of Beni Hasan necropoli were reused as cemeteries at the locus. Other genes white markings allow point coloration are more frequent in the palace in order to optimize the link between two of the remote village is accessible from Houtong Station of the cat gap."
  },
  {
    "objectID": "posts/catGPT/index.html#closing-remarks",
    "href": "posts/catGPT/index.html#closing-remarks",
    "title": "CatGPT: Part 1",
    "section": "Closing remarks",
    "text": "Closing remarks\nWell, this has been genuinely fun. Our little chat bot may not be the sharpest tool in the shed, but he’s certainly… something. A bit darker than I expected. Anyway, thanks for getting through this with me, if you’ve made it this far. Next time around we’ll take this one step further, by building a Flask web app for our beloved CatGPT, hosting it using Google Cloud’s App Engine, and building a JavaScript frontend so that all can enjoy his depressing cat facts on this site. It’s gonna be fun."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Bassel Saleh, I’m 26 years old living in Austin, Texas, and I’m currently pursuing a Ph.D. in Computational Science, Engineering, and Mathematics (CSEM) at the University of Texas. I am in my fifth (and hopefully final) year in the program, and as explained on the home page, I created this blog to synthesize some of the things I’ve learned and work on developing a voice that is both technical and personal.\nI received two Bachelors of Science, one in physics and the other in computer science, also from the University of Texas. I earned a Master’s degree in CSEM after completing the coursework for my PhD program.\nMy full CV can be found here."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "About",
    "section": "Research Interests",
    "text": "Research Interests\nMy interests lie in the space where computation meets interesting physics. A strong theme in my research has been quantitification of uncertainty, especially as it relates to solving Bayesian inference problems with expensive or biased forward models. My main line of research, that which pertains to my dissertation, is about solving the inference problem in gravitational wave data analysis. In short, I get to study the astrophysics of colliding black holes in extreme gravitational environments, while focusing on the general challenges of performing numerical inference, namely the problem of sampling from probability distributions in high dimensions.\nI also work on scientific machine learning. Data-driven solutions to modeling complex systems in physics and engineering are gaining in popularity, largely because of how computationally efficient these methods can be. However, in critical systems (e.g. jet plane autopilot, nuclear reactor management, extreme weather prediction), the consequences of innacurate or untrustworthy model predictions can be catastrophic, so proper quantification of uncertainty is a necessity. This is the second primarily line of my research."
  },
  {
    "objectID": "about.html#personal-interests",
    "href": "about.html#personal-interests",
    "title": "About",
    "section": "Personal Interests",
    "text": "Personal Interests\nAlright, I’m gonna be honest, I tried to use ChatGPT to write this section because I didn’t know how to express myself. The truth is that outside of my professional pursuits, my interests are quite broad and kind of all over the place. In my free time I tend to get sucked into Wikipedia rabit holes and podcast deep dives, a habit I hope that I share with many of the readers of this page. It’s very much in the spirit of climbing the giant, to pursue knowledge in as many fields as possible, while remembering to “follow the fun” and not get too haughty about it. Broadly I’d say I’m most interested in history and linguistics, in the sense that if I were barred from a career in STEM, those would be the subjects I’d be most excited to engage with.\nOutside of the high brow stuff, my hobbies include soccer1, video games, chess, and trying my best to spend quality time with friends and family. My favorite season is summer because I love to swim and enjoy the sun. In Austin, summer means eating tacos under clear skies, taking a dip in the natural springs and watering holes of central Texas, and hiking through lush greenbelts with my friends.\n\nFinally, I love to travel. From Covid-era road trips to hike through remote national parks to relaxed Mediterannean escapes on the islands of Malta and Cyprus, I’m a firm believer that the more of the world you see the more full a person you become."
  },
  {
    "objectID": "posts_hidden/welcome/index.html",
    "href": "posts_hidden/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts_hidden/first-post/2021-05-03-first-post.html",
    "href": "posts_hidden/first-post/2021-05-03-first-post.html",
    "title": "Hello World!",
    "section": "",
    "text": "So here it is, my first blog post. More likely than not this particular post will have a short life, and I’ll delete it once I create my first hIgH quAliTy blog post. I have used markdown a little bit before, mostly for making hastily thrown together README files for repos that I never made public anyway. It seems pretty intuitive to use so far. Let’s test some features.\n\n\nBig fan of itemization. Thankfully with markdown this is easy to do and make pretty:\n\nFor example, this is the first item of a list.\nThis is the second.\n\n\n\n\nI don’t imagine I’ll be integrating code too often, but perhaps I’ll make some simple tutorials in the future for libraries I like/workflows I find useful.\nHere’s some Python code:\ndef funky(x):\n    return str(x) + 'is a funky monkey'\n\n# will a comment show up?\nprint(funky(6))\nDoes Julia syntax work?\nfunction julia_funk(x)\n    x = \"funkalicious\"\nend\n\n# julia comment\nx = julia_funk(6)\n\n\n\nSo Jekyll uses a markdown parser called kramdown, which supports MathJax. In theory I can render \\(\\) stuff like so:\n\\[\n-\\frac{\\hbar^2}{2m}\\nabla^2\\Psi = i\\hbar\\frac{\\partial}{\\partial t}\\Psi \\\\\n\\]\n\n\n\nOkay final test for now. Let’s insert a big picture of my face. And then figure out how to make it less big later.\n\nTurns out it made the picture the perfect size. This is all working so well so far!\nI’m currently building the site locally and viewing it in the browser using a Jekyll server inside a docker thing… I just followed some tutorials and set it up. Once I push the changes this should be reflected on the live site."
  },
  {
    "objectID": "posts_hidden/post-with-code/index.html",
    "href": "posts_hidden/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Climbing the Giant",
    "section": "",
    "text": "Welcome to my blog! This site is a new endeavor for me, and I’m planning to use it as a place to gather my thoughts on things I’ve learned, have some fun, and hopefully make something that others might find useful. I’ve never managed a website before, nor have I had much experience writing casually for anyone other than myself, so I imagine there will be some growing pains as I get in the swing of things. But I’m looking forward to exercising this muscle and making something new, and if you the reader find a single thing of use or interest on this site, then I will consider this a resounding success.\nBut who am I anyway? My name is Bassel Saleh, and I’m currently a Ph.D. student in computational science, engineering, and mathematics at the University of Texas in Austin. I’m very much at the start of what I hope is a long career in scientific research, and this is my way of summarizing some of the things I’ve learned for other young scientists out there trying to find their footing. Because of my background, this blog will focus mostly on mathematics, physics, and computer science, but expect to find a wider variety of content here as well. From other academic interests of mine like history and linguistics to general musings on the work-life balance of being a graduate student, I’m putting no hard limit on what I permit myself to discuss.\nWant more info about me? Check out the About Me page.\nCurious about the name Climbing the Giant? I explain it all in my first post, Not Knowing.\n\nPosts\n\n\n\n\n\n\n\n\n\n\n\nCatGPT: Part 1\n\n\n\n\n\n\n\nCoding Project\n\n\n\n\nThe dumbest chat bot you’ll see this year.\n\n\n\n\n\n\nFeb 7, 2023\n\n\nBassel Saleh\n\n\n\n\n\n\n\n\nNot Knowing\n\n\n\n\n\n\n\nExplainer\n\n\n\n\n“They say great science is built on the shoulders of giants. Not here. At Aperture, we do all our science from scratch. No hand holding.” - Cave Johnson\n\n\n\n\n\n\nMay 9, 2021\n\n\nBassel Saleh\n\n\n\n\n\n\nNo matching items"
  }
]