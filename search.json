[
  {
    "objectID": "posts/physics-time-loops/index.html",
    "href": "posts/physics-time-loops/index.html",
    "title": "The Physics of Time Loops",
    "section": "",
    "text": "“What was that noise?” I asked myself as I spun wildly around looking for the source of the loud bang. I had just spent the past half hour or so travelling around the solar system in awe, carefully twisting and turning my spaceship to land on exotic planets and bouncing gleefully around them in my jet-powered spacesuit looking for clues. It wasn’t long before I located the source of the bang, a massive explosion glowing bright blue, expanding out from where the sun had been moments before. When the powerful blast washed over me and obscured my vision, I thought perhaps it was all over, but moments later I awoke with a sharp breath, staring up at the night sky from the safety of my cozy home planet Timber Hearth. I was standing in exactly the same place I had been at the start of my adventure, the sun still intact and everything else exactly where it had been.\nThis was the delightful experience of playing the video game Outer Wilds for the first time. After going through the loop several times, I found myself starting each one with a greater sense of urgency, trying my hardest to get to the next location and uncover the next clue before the inevitable supernova would send me back to the start. I became much more efficient at flying my spaceship and hurtling past early obstacles on my path to uncover the secrets of the universe. You see, time loops can be a powerful device for storytelling. For video games in particular, putting the player in the shoes of someone repeating the same experiences over and over again, with the mission to try something new to change the loop, creates frustration and exhiliration. It encourages the player to engage with themes like free will and the futility of problem solving when control is repeatedly taken away from you.\nBetween games like Deathloop, Returnal, and 12 Minutes, time loops appear to be a popular mechanic in video games recently. Of course, the concept of a time loop is significantly older and more medium-cutting than the current video game boom, from classic films like Groundhog Day and Edge of Tomorrow to the 1915 Russian novel Strange Life of Ivan Osokin. My experience with Outer Wilds in particular got me thinking about what the rules are in this universe that would allow such a story to be logically consistent. And how different do these rules need to be from the laws of physics governing our universe, the real one that we all live in. Do the laws of physics allow for time loops to plausibly exist? If not, in what ways would the laws need to be modified to permit them? Is there a way of formulating physical laws, even in principle, that could permit time loops without disintegrating into a pile of paradoxes and contradictions?\nOf course, if you want to be a bit glib, you could argue that this whole avenue of thinking is silly. These are fictional stories after all, so who cares about the minutia of physics in analyzing them. The thing is, I kind of agree. Science fiction stories can be wonderful and enjoyable even when their physical worlds bear no resemblance to the one we live in. But I happen to also believe that the interplay between true science and science fiction is both fascinating and useful, for finding new interesting stories to tell as well as finding new scientific avenues to pursue. So for now we will proceed under the assumption that asking such questions might lead us to interesting places, and let our imaginations do the rest."
  },
  {
    "objectID": "posts/physics-time-loops/index.html#the-red-herring-of-closed-timelike-curves",
    "href": "posts/physics-time-loops/index.html#the-red-herring-of-closed-timelike-curves",
    "title": "The Physics of Time Loops",
    "section": "The Red Herring of Closed Timelike Curves",
    "text": "The Red Herring of Closed Timelike Curves\nI looked up “the physics of time loops” and found lots of popular science articles on the subject, including one that interviewed one of the game designers of Deathloop. While several of these articles were quite engaging to read, I was a bit disappointed to find little more than a cursory reference to some physics here and there, rather than a detailed discussion of how these physical theories would best explain time loops. Many articles jumped right to talking about closed timelike curves (CTCs), which are a mathematical feature of some very specific solutions to Einstein’s equations for general relativity. In the somewhat broader discussion of more general time travel in science fiction, CTCs come up a lot, as they allow for a relatively straightforward way to allow one to travel to the past. More precisely, a CTC is a path in curved spacetime, such that if you hopped in a spaceship and travelled along it, you would end up back where you started, in both space and time. This isn’t through some magical teleportation or backward traversal, it’s simply a quirky consequence of extremely curved spacetimes, some of which are technically allowed in general relativity.\nThere are issues with taking CTCs too seriously, and some of my thoughts on this were informed by Sean Carroll’s excellent podcast episode about time travel, where he investigates in detail what some of these issues are. But for time loops specifically, I believe pointing the discussion toward CTCs is a mistake. CTCs exist theoretically as part of what Carroll calls a “single consistent universe”. If you were to use a CTC to travel backward in time, you would not find that events are any different than the last time you went through them. There are not alternate versions of events that you’re free to pick between each time you experience the loop, there is just the one universe, one sequence of concrete events, that you happen to traverse over and over again. This is just completely at odds with the kind of time travel that exists in time loop stories. In the latter, it seems more like time is something that can be unwound, events “undoing” themselves or otherwise resetting, such that choices you make this time induce slightly different initial conditions that can propagate into wildly different outcomes. For this to work, the player/protagonist must exist outside of the time-unwinding mechanism, or else all their memories would also be unwound and the decisions they make would be exactly the same.\nOther small issues with the CTC picture exist as it relates to time loops. For one thing, if a time loop is to be the result of traversing a CTC, it doesn’t seem logically possible for it to depend in some way on the experience of the character. It’s a lot easier to make sense of a loop if it exists for a fixed period of time before resetting. Some time loops seem to be of variable length, such as in Groundhog Day and Palm Springs, where characters only reset the loop when they fall asleep or die. Many video game loops are similar, where some unpredictable event (like death) can trigger the loop. Outer Wilds, even though it has this death = loop property, actually does pretty well at avoiding the issue, since the sun goes supernova after some fixed, constant amount of time no matter what. With this property, the loop seems less attached to the conscious experience of the protagonist. You could imagine for example that if the protagonist dies, the rest of the universe chugs along until the sun explodes once more, and only then does the protagonist get sent back to the start of the loop and regain consciousness.\nThere’s also a pretty solid consensus that CTCs, despite being theoretically allowed by the equations of general relativity, are practically impossible to ever create in the universe, either because of the need for negative mass, or infinite mass, or other exotic features like cosmic strings and wormholes. But we don’t even need to defer to these issues to dismiss CTCs as being relevant for time loop stories. They just don’t produce the desired narrative effect."
  },
  {
    "objectID": "posts/physics-time-loops/index.html#winding-back-the-clock",
    "href": "posts/physics-time-loops/index.html#winding-back-the-clock",
    "title": "The Physics of Time Loops",
    "section": "Winding Back the Clock",
    "text": "Winding Back the Clock\nSo if the CTCs of general relativity can’t explain time loops, is there anything in physics that can? I mentioned the time unwinding idea, but that is sort of vague and magical. Real macroscopic events in the universe are not time reversible, due to the second law of thermodynamics. Ok so fine, as writers trying to craft an interesting narrative, what ideas can we lean on to at least be consistent? In principle, due to the time reversal symmetry of the fundamental laws of physics, if we had some way of going through every particle in the universe and pointing their momenta in the exact opposite direction, we could press play again and watch everything perfectly undo itself, throwing a big middle finger to the second law as we do so. Then of course once a desired state is reached (the start of the loop), we would need to pause and go flip all the momenta again so that events could go forward from there. Unfortunately, we have to somehow exempt the particles that make up the protagonist from this process, at least from their potential death on backward. I suppose we also need to knock them unconscious while all the unwinding is happening, or else they would witness and have memories of everything else going on. So ok fine, at some specified event (the sun exploding, for example), we grab the protagonist, knock them out, isolate them completely from everything else in the universe, go through and flip all the particle momenta so that the past 22 minutes or whatever perfectly unwinds, go through and flip all the momenta again once we hit the start of the loop, then throw the protagonist back into the universe to wake up and wonder how the hell they’re reliving the same day.\nThis seems… at least plausible (in a classical universe anyway), but I can’t stress enough just how immense a power would be needed to do this. Maybe we don’t have to go mess with the entire universe, if the solar system is isolated enough and the time interval we’re reversing is short enough, but still. This takes a nearly Laplace’s demon level of knowledge and computing power, not to mention the unfortunate reality that all of the momentum flipping has to happen at exactly the same time, which is troubling because the concept of simultaneity is its own can of worms. Still, even if it’s practically speaking stupendously difficult, this is at least a logically consistent mechanism for the time loop to happen, especially since the intelligence playing Laplace’s demon in this example could just take the protagonist to some other solar system while all the resetting happens and then bring them back.\nThe above scenario gets messier when you acknowledge that the universe isn’t actually classical, it’s quantum mechanical. It is not possible to go flip the momenta of all of the particles, because they do not have well-defined momenta at all. Furthermore, in the standard model of particle physics the fundamental laws don’t actually have nice, clean time reversal symmetry. They have what is called CPT invariance, where the laws only function normally if you reverse time and flip the charge of all particles and flip the polarity of all particles (swap right and left handedness, so to speak) at the same time. But ok fine, you do all that… somehow. The problem is that in quantum mechanics, the observable thermodynamic evolution of the universe is not deterministic, at least not to any one observer. Quantum outcomes will occasionally be random, such that if you hit restart on the universe and watched it unfold from the exact same initial condition as before, you wouldn’t necessarily observe the exact same unfolding of events. So either you need to force the quantum measurements to somehow reproduce the same results they had last time (nothing in our understanding of quantum mechanics suggests this is even remotely possible), or you need to prepare yourself to make the final plunge: into the universe of many worlds."
  },
  {
    "objectID": "posts/physics-time-loops/index.html#you-get-a-world-and-you-get-a-world",
    "href": "posts/physics-time-loops/index.html#you-get-a-world-and-you-get-a-world",
    "title": "The Physics of Time Loops",
    "section": "You Get a World, and You Get a World",
    "text": "You Get a World, and You Get a World\nIf you aren’t aware, there isn’t just one theory of quantum mechanics, there are several different competing formulations. These different theories are ontologically distinct, saying different things about what the universe fundamentally is like, but they all agree with each other when predicting the outcomes of experiments. For this reason, some modern physicists disagree that they truly constitute different scientific theories, and there’s a whole philosophical debate on this subject, if you’re curious. For our purposes we’re going to adopt the so-called many-worlds formulation of quantum mechanics, since this is the physical theory which unlocks the pantheon of parallel universes that might save us in our pursuit of time loop physics.\nIn simple terms, many-worlds posits that there is only one thing that is real, the quantum mechanical wavefunction of the universe, and this wavefunction evolves according to the Schrödinger equation. Quantum mechanical orthodoxy demands that this wavefunction collapse to one discrete outcome when a measurement is made, but in many-worlds, we do away with the concept of collapse and replace it with the concept of branching. Whenever a quantum measurement occurs that could have gone one way or another, the universe splits into two, or three, or maybe infinite copies of itself, where each copy proceeds with one of the outcomes.1 These parallel copies are branches of the wavefunction, and believing in them allows one to keep an internally consistent picture of the world.\nThis is all perfectly kosher, in the sense that many-worlds (or Everettian) quantum mechanics is a genuine, bonafied physical theory, with equations and everything, one that is consistent with the universe we actually live in. However, before you get excited about jumping around to alternate timelines to meet different versions of yourself, it’s worth knowing what is and is not possible in this theory. For one thing, many-worlds says that once a branching of the wavefunction happens, that’s it. The branches no longer interact or have any causal influence over each other, and there is no way to travel from one to the other.\nBut ok fine, if we really want to make time loops happen (and we do, if we are writers trying to tell a story), let’s say we can invent some mechanism that lets a person (or spaceship) jump between branches of the wavefunction. Ignoring for the moment any wild paradoxes that might create, can we at least imagine that such a thing were possible? If we let ourselves do this, what would such a person’s experience be like? Would it feel the way it felt to Bill Murray in Groundhog Day or to the spacefaring protagonist of Outer Wilds?"
  },
  {
    "objectID": "posts/physics-time-loops/index.html#winding-back-the-quantum-clock",
    "href": "posts/physics-time-loops/index.html#winding-back-the-quantum-clock",
    "title": "The Physics of Time Loops",
    "section": "Winding Back the… Quantum Clock?",
    "text": "Winding Back the… Quantum Clock?\nWell, not quite. There is one more sticking point, which is that if we’re jumping between branches, that doesn’t let us choose what moment of time we’d like to jump to. Really what we need is to travel backwards to a point before the branching. So it turns out we actually need to refer back to our old thermodynamic argument for unwinding events, but this time instead of flipping momenta to reverse time like Laplace’s demon, we need our grand intelligence (perhaps called Everett’s demon) to unwind the quantum mechanical wavefunction of the universe, which is to say unwind all of the branches together, using CPT invariance to ensure it all stays consistent. This effectively undoes the series of quantum entanglements that caused the wavefunction to branch, bringing us back to a point where we can pause, take a breath, and apply the CPT transformation once more to point us back in the right direction. In a very real sense, this is exponentially harder than the already insane task of reversing particle momenta in the classical universe, and unlike when we could imagine dragging our unconscious protagonist to a nearby solar system, now we need our protagonist to somehow sit outside of the wavefunction of the universe? That doesn’t seem plausible at all. But in my opinion it’s at least better than pretending quantum mechanics doesn’t exist, so it’s about as good as we’re going to get. So with all that said… now can we answer whether this wacky universe would behave like Groundhog Day, Outer Wilds, Deathloop, etc.?\nIn principle, yes… I think. This is where not being an expert starts to hinder us, because the details of how such a bizarre thing might work are murky and unreliable. At some point, you have to start putting pen to paper and ask yourself what the equations tell you. Scientific theories are not just collections of ideas, they have well-posed mathematical structure, and we better be careful about messing with that structure lest we accidentally break everything else about the universe. But in my humble opinion, considering the discussion we’ve had so far, this is the version of time looping physics that makes the most sense to me, or at least creates the least amount of trouble. You have to do a lot of hand waving about how one might travel outside of their own branch, and you might have to do some careful tweaking to make sure you don’t end up somehow inside of yourself or otherwise conflicting with matter on another branch, but if we suspend a little bit of disbelief, then this is the physical theory that has the most explanatory power for justifying a genuine time loop. It’s the only one that lets us make sense of the fact that each loop feels different, with different actions leading to different outcomes. From a story perspective, this ingredient is essential, and you could never get it with closed timelike curves alone.\nSean Carroll claims that the “right” way to do time travel in science fiction is with the single, consistent universe idea. Which is to say, no matter what kind of time-faring shenanigans you get up to, whether it’s CTCs or something else, you’ll never exert causal influence in a way that makes things happen any differently than they already happened. This is in some sense the most physics-pleasing way of doing time travel, but unfortunately, with time loop stories this explanation was dead from the word go. That is what sent me on this rabbit hole of trying to find the next best explanation. But please, if you have some idea that you think works even better, or if you find a flaw in the arguments I’ve given here, let me know! I can always be reached on Twitter @mrbassoule."
  },
  {
    "objectID": "posts/not-knowing/2021-05-09-not-knowing.html",
    "href": "posts/not-knowing/2021-05-09-not-knowing.html",
    "title": "Not Knowing",
    "section": "",
    "text": "I don’t have any children, but if I did I think I’d be one of those parents who keeps kicking the naming can down the road and referring to their newborn as “the baby” for weeks into its life. Point being, I think naming something is really hard, especially if it’s something young and new that hasn’t quite fully formed. I overthink and second guess, I make spreadsheets of options and weigh the pros and cons, and in the end I can never be 100% satisfied with the result. I also think this defficiency is not unique to me in my profession, for it seems many practicioners of science struggle to creatively christen their creations. I think if academic papers were required to have short, uncomplicated titles, there would be a lot less getting published every year.\nIn the epic struggle of picking a name for this blog, I settled on a reference to the famous quote by Isaac Newton:\n\nIf I have seen further than others, it is by standing upon the shoulders of giants.\n\nI think this quote captures the essense of what I’m trying to do with this blog. It’s a powerful metaphor, the idea that science allows us to reach far beyond our individual acuities, to glimpse profound truths on the horizon of human understanding, but only because we are elevated by the insight and hard work of generations of past thinkers. It’s beautiful, in a way, to imagine perching precariously on the shoulder of some enormous creature, gazing at a vast landscape that would be hidden to those on the ground.\nBut it’s also a bit… grandiose? Pretentious? Who am I, after all, to suggest that my shoulders are worth standing on? I’m no giant, no leading expert in any field. I’m not even a full-fledged Ph.D. yet. And thus the appropriate version of this metaphor became apparent to me. I’m not an authority on science or mathematics, or anything else for that matter. But I’m a student, an aspiring shoulder-gazer. I’m still standing on the ground, looking up at the giant and wondering about the view.\nSo by reading this blog you won’t necessarily get the perspecive of a giant’s shoulders, but you can, if you like, come along with me while I start to climb. Every day we’ll get a little higher and see a little further. I hope to make a career some day out of science communication, so maybe this is the natural place to start, at the giant’s feet, climbing gear in hand. Maybe some day we can look out at the horizon together."
  },
  {
    "objectID": "posts/not-knowing/2021-05-09-not-knowing.html#a-little-time-to-change",
    "href": "posts/not-knowing/2021-05-09-not-knowing.html#a-little-time-to-change",
    "title": "Not Knowing",
    "section": "A Little Time to Change",
    "text": "A Little Time to Change\nThere’s another problem with Newton’s quote about the giants, besides it being a bit lofty. It’s basic. Many of you I’m sure have heard the quote before. It’s appeared numerous times across popular culture, from song lyrics to sitcoms. Stephen Hawking wrote a whole book called On the Shoulders of Giants: The Great Works of Physics and Astronomy. The official motto of Google Scholar is “Stand on the Shoulders of Giants”, and the video game Portal 2 humorously subverts the metaphor through the scientific arrogance of the fictitious Aperture Science. It’s even inscribed on the edge of the British two pound coin.\nPerhaps it’s the dormant Austin hipster in me that I pretend isn’t there, but surely you don’t want something too popular as the source for your name. It’s not cool unless it’s sufficiently obscure, right? Well there is another quote, one that I think even better represents my attitude toward this blog (and also probably toward life in general), and it is at least slightly more obscure:\n\nI was born not knowing and have had only a little time to change that here and there.\n– Richard Feynman\n\nIt’s hard to convey just how much this sentiment resonates with me. It’s direct and unpretentious, and it boils all of the complexity of life down to something simple but true: we are born ignorant, and every day we gather data to update our knowledge of the world. Every exam taken, every book or social media post read, every conversation had, and every sight seen and sound heard contributes to this endeavor. And we are all equal in this way, all born the same and then thrust into the world to absorb as much as we can before our time is up.\nBut alas, partly due to the simplicty of its language that drew me to it, this quote does not contain a catchy two or three word phrase that can be synthesized into the name of a blog. Trust me, I tried. I settled on referencing it in the title of this post, but that’s about all I could milk out of it. So instead you get the story about the giants and the legacy of Isaac Newton and every scientist, songwriter, and video game developer who’s quoted him over the years.\nOh well, I can live with being a little basic, especially since this was all just an excuse to ramble about interesting quotes and the several sources of inspiration I had for starting this blog. Maybe that’s the best summary of what to expect: me looking for excuses to rant about things I find interesting.\nDid you know Newton didn’t even come up with the shoulders of giants thing? When he mentioned it in a letter to Robert Hooke in 1676, he was referencing a metaphor that was already quite popular at the time, one which apparently can be traced to Bernard of Chartres, a 12th century French philosopher, who had in mind the image of dwarfs riding on the shoulders of giants from ancient Greek mythology. I wonder what Bernard would think of all the credit Newton gets for referencing his metaphor."
  },
  {
    "objectID": "posts/catGPT/index.html",
    "href": "posts/catGPT/index.html",
    "title": "CatGPT: Part 1",
    "section": "",
    "text": "Well, I’m back. And after nearly two years of no activity on this blog, what incredible spectacle do I have prepared for you, my irrationally loyal reader? Introducing CatGPT, the Catenated Grimalkin Program Talker. As stupid as it sounds, I spent a lot of time with the thesaurus to come up with that name, so please let me have this. Naming things is hard.\nCatGPT is a simple AI chat bot that uses Markov chain-based text prediction to generate responses, and it seems to have suspiciously specific knowledge about just one thing… cats. So like, do you get it now? Grimalkin is an old word for cat, you see. And if you think that’s silly then you try googling “synonyms for cat that start with G, P, or T” and see if you can do better. On second thought, you might want to skip the “P” search. Anyway, catenate is a word containing “cat” that also happens to mean “to link together”, which fits well with the Markov chain theme, so yea, this name is actually very clever, and it’s you who’s spent too much time thinking about it.\nWhere was I? Oh yes, CatGPT isn’t just a blatant attempt to capitalize on the current popularity of some other chat bots out there, it’s one that we’re going to build together! I guess to be more clear, this is a tutorial where I’ll walk you through how to build a text generation tool using simple Markov chains and dress it up a bit to be something we can talk to. Obviously we won’t be blowing any LLMs out of the water here, but hopefully we’ll learn some things along the way about probability, natural language, and Python development."
  },
  {
    "objectID": "posts/catGPT/index.html#markov-chains-and-language",
    "href": "posts/catGPT/index.html#markov-chains-and-language",
    "title": "CatGPT: Part 1",
    "section": "Markov chains and language",
    "text": "Markov chains and language\nIn a fairly abstract sense, text generation comes down to answering the question: how do I decide what to say next based on what was just said? Language, after all, is not random; it contains structure (grammar, syntax, diction) that emerges as correlations in the text. Certain characters are more likely to follow certain other characters, which are more likely to be arranged in certain orders and appear in certain places, and so on. This correlation structure is what allows language to carry information, such that the reader can parse that information using his or her own understanding of the structure. In general, natural human language is messy and imprecise, and the actual information content of a piece of text is highly complicated, involving correlations that aren’t necessarily local. One of the things that makes ChatGPT so powerful is its ability to capture and reproduce these correlations at many scales, allowing for text that feels holistic and sensible from start to finish.\nWe, however, will start from a humble place and focus on the kind of text generation where what we say will depend only on what was just said, roughly speaking. It is precisely this property that will make our model Markovian, i.e. the probability that we produce a character \\(c\\) will depend only on the previous \\(k\\) characters in the text. You might think that we need to restrict ourselves to looking at only one previous character to be truly Markovian, but it turns out the number of characters doesn’t really matter (in fact, we could do the same thing with whole words or even more abstract “tokens”, if our source text were rich enough). What matters is that our next character is influenced by a sequence of previous ones, not that sequence and some other sequence from a couple paragraphs back, for example. In this way, one sequence influences the next sequence, which influences the next sequence, and so on such that all the sequences of length \\(k\\) are probabilistically linked together, which is what makes this a Markov chain.\nSo, we’ll call \\(k\\) the kernel size and call the output (single character) of our model \\(y\\). Then, given an input \\(x\\) that is a string of \\(k\\) characters, we want to compute the probability \\[ P(y = c | x)\\quad \\forall c\\in\\mathcal{A} \\] Here, \\(\\mathcal{A}\\) is the alphabet for our model, the set of all the possible characters we’re allowed to produce. In our case this happens to also be all the unique characters that appear in our source text. Once we have this probability in hand, we’re golden. For a given \\(x\\), we have a probability distribution over all possible choices of \\(c\\). All we need to do is pick a character at random according to that probability, spit it out, and repeat.\nThe trick, of course, is that we need to somehow compute this probability. In our case, this will be done by simply counting occurences in the original text, such that our generated text will have the same correlation structure as the source, for whatever \\(k\\) is pre-selected."
  },
  {
    "objectID": "posts/catGPT/index.html#developing-the-code",
    "href": "posts/catGPT/index.html#developing-the-code",
    "title": "CatGPT: Part 1",
    "section": "Developing the code",
    "text": "Developing the code\nFor this project, we’re going to use Python (for now). It’s worth noting that there are powerful tools out there for manipulating text and processing natural language, such as NLTK, but we’re going to mostly avoid those for this project and do things ourselves.1 We’ll start by building up the three main ingredients of our model:\n\nBuilding the probability distribution from the source text\nGenerating new text\nMaking it interactive\n\n\nBuilding the distribution\nThere are different ways to go about this, but the first idea I had turned out to be easy to implement and perfectly adequate in efficiency. The idea is to use nested dictionaries to create a lookup table for character frequencies. The outer dictionary’s keys will be sequences of \\(k\\) characters, each one pointing to an inner dictionary of characters pointing to their frequencies.\ndef build_lookup(source_text, k):\n    lookup = dict()                     # the outer dictionary\n\n    for i,_ in enumerate(source_text):\n        if i+k >= len(source_text):     # make sure you don't go out of bounds\n            break\n\n        x = source_text[i:i+k]          # x, the current sequence of k chars\n        y = source_text[i+k]            # y, the next char that appears\n\n        if x not in lookup:             # if this is the first time we've seen x, add it\n            lookup[x] = {y: 1}\n        else:\n            if y not in lookup[x]:      # count the number of times y appears after x\n                lookup[x][y] = 1\n            else:\n                lookup[x][y] += 1\n    \n    return lookup\nThe loop creates a sliding window, our \\(k\\)-sized kernel, that scans over the source text and counts up the number of times different characters follow. Now, when we access this lookup dictionary with some input x, what we get back is the probability distribution we were after. Of course, really what we get is a distribution of frequencies, and we’d need to divide these by the total number of occurences to get probabilities, but this is an unnecessary step, as we’ll see later. Python’s random.choices function allows us to use the raw frequencies as weights to get the desired result.\n\n\nGenerating new text\nIt almost seems too good to be true, but generating new text is similarly pretty straightforward. What we need is a desired length for the new text, a seed from which to start adding characters, and the lookup table that we built from before. We just pick new characters from the lookup inner dictionary weighted by their frequencies (which are the values in that dictionary).\nimport random\n\ndef generate_text(lookup, k, length, seed):\n    text = seed\n\n    for i in range(k, length):\n        x = text[i-k:]                      # current input, starting with seed\n\n        chars = list(lookup[x].keys())      # the chars to choose from\n        weights = list(lookup[x].values())  # the frequencies with which those chars appeared in source\n\n        next_char = random.choices(chars, weights=weights, k=1)[0]\n        text += next_char\n    \n    return text\nIt’s really that simple. By the way, random.choices returns a list, so with k=1 we’re telling it to return one character, but we still need to grab the \\(0\\)th element to access it.\nNow would be a great time to run a little test to see if things work. For now, let’s take just the first paragraph from the Wikipedia page for cat and use that as source text.\nsource_text = 'The cat (Felis catus) is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae and is commonly referred to as the domestic cat or house cat to distinguish it from the wild members of the family. Cats are commonly kept as house pets but can also be farm cats or feral cats; the feral cat ranges freely and avoids human contact. Domestic cats are valued by humans for companionship and their ability to kill rodents. About 60 cat breeds are recognized by various cat registries.'\n    \nk = 1\n\nlookup = build_lookup(source_text, k)\n\nseed = source_text[:k]\nnew_text = generate_text(lookup, k=k, length=100, seed=seed)\n\nprint(new_text)\nExecuting the above (\\(k=1\\)) gave me:\n\nThed t bus famo inivats It dond avavamby mmis. acon by 60 f tis hegicon tst wie ican d t omarouivous\n\nIf we crank \\(k\\) up a bit, to say \\(k=5\\), we get something a little more English sounding:\n\nThe cat ranges freely and their ability to kill rodents. About 60 cat ranges freely and their abilit\n\nOur bot seems a bit confused and is repeating itself, but that’s just because of how small our source text is. If you’d like, you can throw an even shorter source text in there, like “hello there”, and print out the lookup table to convince yourself of what it’s doing. This can be useful for debugging.2\n\n\nMaking it interactive\nAs a final step, we’d like to make this text generation code something we can actually interact with. In the spirit of mimicking ChatGPT, we envision a setup where we can ask the bot something, and have its response in some way incporporate what we asked. Furthermore, we’d like to rinse and repeat this to have an ongoing conversation.\nLet me be clear: we’re not getting fancy here. The beloved CatGPT that we’re building is… special. The goal here is to be a bit goofy, so we’re going to make this interactive in quite a dumb way. But more on that later.\nFirst, let’s fill CatGPT’s brain with a lot more info, specifically all the Wikipedia pages in the category “Cats”. The general and proper way to scrape data from the web would be to use a combination of the requests and beautifulsoup packages, but we can get away with even more simplicity by using the wikipediaapi package. If we want to extend our bot to learn from other online sources in the future, we’d need to go the former route.\nSo let’s write a function that takes a category name and grabs the content for all the pages that are in that category.\nimport wikipediaapi\n\ndef load_wiki(category):\n    wiki = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI)\n\n    cat_page = wiki.page(category)\n    article_names = [p for p in cat_page.categorymembers.keys() if ':' not in p]\n\n    content = ''\n    for article in article_names:\n        page = wiki.page(article)\n        page_content = page.text\n        content += page_content\n\n    return content.replace('\\n', ' ')\nThe one subtlety to point out here is in the line article_names = [p for p in cat_page.categorymembers.keys() if ':' not in p]. We’re excluding page names that contain “:” in them because we don’t want to deal with the recursive nature of Wikipedia categories. To keep things simple, we’re only going to scrape the pages that sit directly in the “Cats” category, and this line of code seems to do the trick.\nNow the final step. We need to:\n\nBuild our lookup table using the source text scraped from the Wikipedia articles\nAsk the user for some input\nParse the input to extract “keywords” to then search for in the source text\nRandomly select a \\(k\\) sized seed from the text based on this keyword search\nGenerate a response using that seed\nRepeat\n\nOk maybe I got a bit ahead of myself when I said “final step”. Steps 3 and 4 here seem like a head scratcher, so let’s spell out a little more clearly what they mean. Again, I want the user’s input to somehow influence the response by CatGPT, and the simplest way I could think to do this was to search the source text for instances matching words in the user’s input and use that as a seed to generate the response.\nTwo issues immediately arise from this approach. First, we don’t want to waste our time looking for common words and articles, so we’re going to use a dictionary of stopwords from nltk.corpus to filter out such words, leaving us with more interesting “keywords”. Second, we need our seed to be of length \\(k\\), so some padding around the keywords will be needed to make sure whatever sequences we find are the correct length. Then we’ll just pick one at random and go from there.\nFinally, the code to do all this is below. There are some small tricks here and there, like using regex and string translation to filter out punctuation, prepending our keywords with spaces to avoid finding matches in the middle of words, and some others. But instead of going through every trick, I encourage you to take this code and play around with it yourself to uncover all its quirks. Again, for a project like this it is absolutely necessary to continually unit test, try your best to break it with nontrivial inputs, and generally mess around with it until you can shake all the bugs out.\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\ndef run_catGPT():\n    # load text from all the Wikipedia pages on cats\n    source_text = load_wiki('Category:Cats')\n\n    # use nltk to download the set of stopwords\n    nltk.download('stopwords')\n    stops = set(stopwords.words('english'))\n\n    # set kernel size. I'm a fan of k = 10\n    k = 10\n\n    # build probability distribution as lookup table\n    lookup = build_lookup(source_text, k)\n\n    print('\\nHello, my name is CatGPT. What can I help you with today?')\n\n    while True:\n        # request user input, and exit if the user says goodbye\n        input_text = input('\\n> ')\n        if input_text.lower() == 'goodbye':\n            break\n        \n        # get a list of the user's words, ignoring punctuation, then filter out the stopwords\n        words = re.findall(r\"[\\w']+|[.,!?;]\", \n            input_text.lower().translate(str.maketrans('', '', string.punctuation)))\n        keywords = list(filter(lambda w: not w in stops, words))\n\n        # for each keyword, find matches in the text and pad them to be of length k\n        # add them to a list of potential seeds\n        seeds = []\n        for w in keywords:\n            for match in re.finditer(' ' + w, source_text):\n                s = match.start() + 1\n                e = match.end()\n                padded = source_text[s:e]\n                \n                if len(padded) > k:\n                    seeds.append(padded[:k])\n                else:\n                    while len(padded) < k:\n                        e += 1\n                        padded = source_text[s:e]\n                    \n                    seeds.append(padded)\n        \n        # if for some reason no match was found, set the seed as start of text\n        if not seeds:\n            seed = source_text[0:k]\n        else:\n            seed = random.choice(seeds)\n\n        # generate a response of length 500 (or whatever you want)\n        response = generate_text(lookup, k, length=500, seed=seed)\n\n        # do some post-processing to make sure the response ends with a period and starts capitalized\n        last_period = response.rfind('.')\n        if last_period == -1:\n            last_period = len(response)\n            response += '.'\n        response = response[0].upper() + response[1:last_period+1]\n\n        print('\\n' + response)"
  },
  {
    "objectID": "posts/catGPT/index.html#a-conversation-with-catgpt",
    "href": "posts/catGPT/index.html#a-conversation-with-catgpt",
    "title": "CatGPT: Part 1",
    "section": "A conversation with CatGPT",
    "text": "A conversation with CatGPT\nThe following is a conversation I had with CatGPT. Enjoy.\n\n\nHello, my name is CatGPT. What can I help you with today?\n\n\n\nHi CatGPT. Do you have any cat fun facts for me?\n\n\n\nCats and free-ranging domestic and wild cats descended from domestic cat with a large area entirely unsuitable for breeding have red eyes. The point gene allele is a single cat found with intact males, while spayed female cat”, also used to call a cat. The myth is attributed to the slaughter.\n\n\n\nJesus, that got kind of dark at the end\n\n\n\nDarkest of all the boarding catteries. In 2007 market surveys showed that 67% of the mice killed by feral cats may chase. Reproduction. These lineages include pain and swelling around the mid 950s BC onwards. At the times at which a kitten becomes difficulties in distinguishable from domesticated cats. Along with rabbits, some sea birds, and sheep, form the entire growth cycle of contraction and establishment, but the current wildcat population. There are various methods for training the cat.\n\n\n\nI didn’t really understand most of that, but yes training a cat is what I want! I’d love to train my cat\n\n\n\nCats. Young orphaned and how long they were embalmed, coffined and buried in cat cemeteries for mummies offered to Bastet.In the mid 19th century. It was suggested that the earth was experience poor growth and can result in retinal degenerations could reach them only with a small cat species, a factor that has been identified as a domestic cats (mostly unowned) are the top human-caused threat to wildlife, transmission of illness.\n\n\n\nOh, I see. That’s pretty bleak. Also, I’m beginning to suspect you’re not really that smart\n\n\n\nAlso neurological cell types. White cats and other animal welfare. The cat will produce black eumelanin based) fur, caused by the abandonment of pets. The usefulness of TNR are often funded by big businesses to combat the city’s cemetery of Beni Hasan necropoli were reused as cemeteries at the locus. Other genes white markings allow point coloration are more frequent in the palace in order to optimize the link between two of the remote village is accessible from Houtong Station of the cat gap."
  },
  {
    "objectID": "posts/catGPT/index.html#closing-remarks",
    "href": "posts/catGPT/index.html#closing-remarks",
    "title": "CatGPT: Part 1",
    "section": "Closing remarks",
    "text": "Closing remarks\nWell, this has been genuinely fun. Our little chat bot may not be the sharpest tool in the shed, but he’s certainly… something. A bit darker in tone than I expected.\nThe code for this project can be found on my Github here.\nThanks for getting through this with me, if you’ve made it this far. Next time around we’ll take this one step further, by building a Flask web app for our beloved CatGPT, hosting it using Google Cloud’s App Engine, and building a JavaScript frontend so that all can enjoy his depressing cat facts on this site. It’s gonna be fun."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Bassel Saleh, I’m 26 years old living in Austin, Texas, and I’m currently pursuing a Ph.D. in Computational Science, Engineering, and Mathematics (CSEM) at the University of Texas. I am in my fifth (and hopefully final) year in the program, and as explained on the home page, I created this blog to synthesize some of the things I’ve learned and work on developing a voice that is both technical and personal.\nI received two Bachelors of Science, one in physics and the other in computer science, also from the University of Texas in 2018. I earned a Master’s degree in CSEM in 2020.\nMy full CV can be found here."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "About",
    "section": "Research Interests",
    "text": "Research Interests\nMy interests lie in the space where computation meets interesting physics. I work primarily on quantitification of uncertainty, especially as it relates to solving Bayesian inference problems with expensive or otherwise complex forward models. For my dissertation research, I’ve worked on solving the inference problem in gravitational wave data analysis. In short, I study the astrophysics of colliding black holes in extreme gravitational environments, while focusing on the general challenges of performing numerical inference, namely the problem of sampling from probability distributions in high dimensions.\nI also work on scientific machine learning. Data-driven solutions to modeling complex systems in physics and engineering are a very active area of research, largely because of how computationally efficient these methods can be. However, in critical systems (e.g. jet plane autopilot, nuclear reactor management, extreme weather prediction), the consequences of innacurate or untrustworthy model predictions can be catastrophic, so proper quantification of uncertainty is a necessity. This is the second primarily line of my research."
  },
  {
    "objectID": "about.html#personal-interests",
    "href": "about.html#personal-interests",
    "title": "About",
    "section": "Personal Interests",
    "text": "Personal Interests\nAlright, I’m gonna be honest, I tried to use ChatGPT to write this section because I didn’t know how to express myself. The truth is that outside of my professional pursuits, my interests are quite broad and kind of all over the place. In my free time I tend to get sucked into Wikipedia rabit holes and podcast deep dives, a habit I hope that I share with many of the readers of this page. It’s very much in the spirit of climbing the giant, to pursue knowledge in as many fields as possible, while remembering to “follow the fun” and not get too haughty about it. Broadly I’d say I’m most interested in history and linguistics, in the sense that if I were barred from a career in STEM, those would be the subjects I’d be most excited to engage with.\nOutside of the high brow stuff, my hobbies include soccer1, video games, chess, and trying my best to spend quality time with friends and family. My favorite season is summer because I love to swim and enjoy the sun. In Austin, summer means eating tacos under clear skies, taking a dip in the natural springs and watering holes of central Texas, and hiking through lush greenbelts with my friends.\n\nFinally, I love to travel. From Covid-era road trips to hike through remote national parks to relaxed Mediterannean escapes on the islands of Malta and Cyprus, I’m a firm believer that the more of the world you see the more full a person you become."
  },
  {
    "objectID": "posts_hidden/welcome/index.html",
    "href": "posts_hidden/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts_hidden/first-post/2021-05-03-first-post.html",
    "href": "posts_hidden/first-post/2021-05-03-first-post.html",
    "title": "Hello World!",
    "section": "",
    "text": "So here it is, my first blog post. More likely than not this particular post will have a short life, and I’ll delete it once I create my first hIgH quAliTy blog post. I have used markdown a little bit before, mostly for making hastily thrown together README files for repos that I never made public anyway. It seems pretty intuitive to use so far. Let’s test some features.\n\n\nBig fan of itemization. Thankfully with markdown this is easy to do and make pretty:\n\nFor example, this is the first item of a list.\nThis is the second.\n\n\n\n\nI don’t imagine I’ll be integrating code too often, but perhaps I’ll make some simple tutorials in the future for libraries I like/workflows I find useful.\nHere’s some Python code:\ndef funky(x):\n    return str(x) + 'is a funky monkey'\n\n# will a comment show up?\nprint(funky(6))\nDoes Julia syntax work?\nfunction julia_funk(x)\n    x = \"funkalicious\"\nend\n\n# julia comment\nx = julia_funk(6)\n\n\n\nSo Jekyll uses a markdown parser called kramdown, which supports MathJax. In theory I can render \\(\\) stuff like so:\n\\[\n-\\frac{\\hbar^2}{2m}\\nabla^2\\Psi = i\\hbar\\frac{\\partial}{\\partial t}\\Psi \\\\\n\\]\n\n\n\nOkay final test for now. Let’s insert a big picture of my face. And then figure out how to make it less big later.\n\nTurns out it made the picture the perfect size. This is all working so well so far!\nI’m currently building the site locally and viewing it in the browser using a Jekyll server inside a docker thing… I just followed some tutorials and set it up. Once I push the changes this should be reflected on the live site."
  },
  {
    "objectID": "posts_hidden/post-with-code/index.html",
    "href": "posts_hidden/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Climbing the Giant",
    "section": "",
    "text": "Welcome to my blog! This site is a new endeavor for me, and I’m planning to use it as a place to gather my thoughts on things I’ve learned, have some fun, and hopefully make something that others might find useful. I’ve never managed a website before, nor have I had much experience writing casually for anyone other than myself, so I imagine there will be some growing pains as I get in the swing of things. But I’m looking forward to exercising this muscle and making something new, and if you the reader find a single thing of use or interest on this site, then I will consider this a resounding success.\nBut who am I anyway? My name is Bassel Saleh, and I’m currently a Ph.D. student in computational science, engineering, and mathematics at the University of Texas in Austin. I’m very much at the start of what I hope is a long career in scientific research, and this is my way of summarizing some of the things I’ve learned for other young scientists out there trying to find their footing. Because of my background, this blog will focus mostly on mathematics, physics, and computer science, but expect to find a wider variety of content here as well. From other academic interests of mine like history and linguistics to general musings on the work-life balance of being a graduate student, I’m putting no hard limit on what I permit myself to discuss.\nWant more info about me? Check out the About Me page.\nCurious about the name Climbing the Giant? I explain it all in my first post, Not Knowing.\n\nPosts\n\n\n\n\n\n\n\n\n\n\n\nThe Physics of Time Loops\n\n\n\n\n\n\n\nExplainer\n\n\n\n\n“Do you ever have déjà vu, Mrs. Lancaster?”\n\n\n\n\n\n\nFeb 9, 2023\n\n\nBassel Saleh\n\n\n\n\n\n\n\n\nCatGPT: Part 1\n\n\n\n\n\n\n\nCoding Project\n\n\n\n\nThe dumbest chat bot you’ll see this year.\n\n\n\n\n\n\nFeb 7, 2023\n\n\nBassel Saleh\n\n\n\n\n\n\n\n\nNot Knowing\n\n\n\n\n\n\n\nExplainer\n\n\n\n\n“They say great science is built on the shoulders of giants. Not here. At Aperture, we do all our science from scratch. No hand holding.” - Cave Johnson\n\n\n\n\n\n\nMay 9, 2021\n\n\nBassel Saleh\n\n\n\n\n\n\nNo matching items"
  }
]